#!/bin/bash

FOLDER_PATH=~/Documents/sync_vault/z-Prompts
BREAKLINES=$(echo -e "\n" && printf "%0.s-" {1..50})

llm_response=$(cat "$FOLDER_PATH/prompt-code.md" | llm -m code -o temperature 0.1 -o max_tokens 4096 -t code_assistant --no-stream)

echo "$llm_response"

logs=$(llm logs -n 1 --json)
id=$(echo "$logs" | jq -r '.[0].id')
model=$(echo "$logs" | jq -r '.[0].response_json.model')
openai_id=$(echo "$logs" | jq -r '.[0].response_json.id')
object_type=$(echo "$logs" | jq -r '.[0].response_json.object')
completion_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.completion_tokens')
prompt_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.prompt_tokens')
total_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.total_tokens')
reasoning_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.completion_tokens_details.reasoning_tokens')
cached_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.prompt_tokens_details.cached_tokens')

# Corrected line for creating a multi-line string
llm_request_log="
**llm id**: $id
**model**: $model
**openai id**: $openai_id
**object type**: $object_type
**usage**:
- completion_tokens: $completion_tokens
- prompt_tokens: $prompt_tokens
- total_tokens: $total_tokens
- reasoning_tokens: $reasoning_tokens
- cached_tokens: $cached_tokens
**Output Text**:"

echo "$BREAKLINES$llm_request_log" >> "$FOLDER_PATH/log/prompt-code-log.md"
echo -e "\n$llm_response" >> "$FOLDER_PATH/log/prompt-code-log.md"
