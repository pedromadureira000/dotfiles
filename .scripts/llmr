#!/bin/bash
FOLDER_PATH=~/Documents/sync_vault/.meta/configs/llm
BREAKLINES=$(printf "%0.s-" {1..50})

prompt_file="1.prompt.md"
response_file="prompt-response.md"
log_file="prompt-log.md"
model="gpt"
# template="default"

# Function to display usage information
usage() {
    echo "Usage: $0 --prompt <prompt-file> --log <log-file> --model <model>"
    # echo "Usage: $0 --prompt <prompt-file> --log <log-file> --model <model> --template <template>"
    echo "  --prompt    : Path to the prompt file (required)"
    echo "  --log       : Path to the log file (required)"
    echo "  --model     : Model to use (required)"
    # echo "  --template  : Template to use (required)"
    exit 1
}

# Parse named parameters
while [[ $# -gt 0 ]]; do
    case $1 in
        --prompt)
            prompt_file="$2"
            shift 2
            ;;
        --response)
            response_file="$2"
            shift 2
            ;;
        --log)
            log_file="$2"
            shift 2
            ;;
        --model)
            model="$2"
            shift 2
            ;;
        # --template)
            # template="$2"
            # shift 2
            # ;;
        *)
            echo "Unknown parameter: $1"
            usage
            ;;
    esac
done

# Check if all required parameters are provided
# if [[ -z "$prompt_file" || -z "$log_file" || -z "$model" || -z "$template" ]]; then
    # echo "Error: All parameters are required."
    # usage
# fi

# Check if prompt file exists
if [[ ! -f "$FOLDER_PATH/$prompt_file" ]]; then
    echo "Error: Prompt file '$FOLDER_PATH/$prompt_file' does not exist."
    exit 1
fi


input_text=$(cat "$FOLDER_PATH/$prompt_file")
if [[ "$model" == *"gemini"* ]]; then
    llm_response=$(echo "$input_text" | llm -m "$model" -o temperature 0.1 --no-stream)
else
    llm_response=$(echo "$input_text" | llm -m "$model" -o temperature 0.1 -o max_tokens 8192 --no-stream)
    # llm_response=$(echo "$input_text" | llm -m "$model" -o temperature 0.1 -o max_tokens 8192 -t "$template" --no-stream)
fi

echo "$llm_response"

logs=$(llm logs -n 1 --json)
id=$(echo "$logs" | jq -r '.[0].id')
model=$(echo "$logs" | jq -r '.[0].response_json.model')
response_id=$(echo "$logs" | jq -r '.[0].response_json.id')
#openai
object_type=$(echo "$logs" | jq -r '.[0].response_json.object')
completion_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.completion_tokens')
prompt_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.prompt_tokens')
total_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.total_tokens')
reasoning_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.completion_tokens_details.reasoning_tokens')
cached_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.prompt_tokens_details.cached_tokens')
#claude
input_tokens=$(echo "$logs" | jq -r '.[0].response_json.input_tokens')
output_tokens=$(echo "$logs" | jq -r '.[0].response_json.output_tokens')

if [[ $model == *"gpt"* ]]; then
  openai_info="**Object type**: $object_type
**openai usage**:
- completion_tokens: $completion_tokens
- prompt_tokens: $prompt_tokens
- total_tokens: $total_tokens
- reasoning_tokens: $reasoning_tokens
- cached_tokens: $cached_tokens"
else
  openai_info=""
fi

if [[ $model == *"claude"* ]]; then
  claude_info="**claude usage**:
- input_tokens: $input_tokens
- output_tokens: $output_tokens
"
else
  claude_info=""
fi

llm_request_log="
# llm id: $id
**Model**: $model
**Response id**: $response_id
$openai_info
$claude_info
## Input Text:
$input_text

## Output Text:
$llm_response
"
echo "$llm_request_log" > "$FOLDER_PATH/log/$response_file"
echo "$BREAKLINES$llm_request_log" >> "$FOLDER_PATH/log/$log_file"
