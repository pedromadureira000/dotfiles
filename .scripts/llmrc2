#!/bin/bash

FOLDER_PATH=~/Documents/sync_vault/z-Prompts
BREAKLINES=$(echo -e "\n" && printf "%0.s-" {1..50})

llm_response=$(cat "$FOLDER_PATH/prompt-code2.md" | llm -m code -o temperature 0.1 -o max_tokens 4096 -t code_assistant --no-stream)

echo "$llm_response"

logs=$(llm logs -n 1 --json)
id=$(echo "$logs" | jq -r '.[0].id')
model=$(echo "$logs" | jq -r '.[0].response_json.model')
# if model contains 'claude' 
openai_id=$(echo "$logs" | jq -r '.[0].response_json.id')
object_type=$(echo "$logs" | jq -r '.[0].response_json.object')
completion_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.completion_tokens')
prompt_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.prompt_tokens')
total_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.total_tokens')
reasoning_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.completion_tokens_details.reasoning_tokens')
cached_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.prompt_tokens_details.cached_tokens')
input_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.input_tokens')
output_tokens=$(echo "$logs" | jq -r '.[0].response_json.usage.output_tokens')

openai_usage="
**openai usage**:
- completion_tokens: $completion_tokens
- prompt_tokens: $prompt_tokens
- total_tokens: $total_tokens
- reasoning_tokens: $reasoning_tokens
- cached_tokens: $cached_tokens"

claude_usage="
**claude usage**:
- input_tokens: $input_tokens
- output_tokens: $output_tokens
"

# Corrected line for creating a multi-line string
llm_request_log="
**llm id**: $id
**model**: $model
**openai id**: $openai_id
**object type**: $object_type
$openai_usage
$claude_usage
**Output Text**:
"

echo "$BREAKLINES$llm_request_log" >> "$FOLDER_PATH/log/prompt-code2-log.md"
echo -e "\n$llm_response" >> "$FOLDER_PATH/log/prompt-code2-log.md"
